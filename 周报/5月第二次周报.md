# 5月第二次周报

时间：2019.5.2 - 2019.5.8

内容：帮学姐跑 NYT 和 SKE数据集、学习强化学习MDP有关知识、了解了一下pytorch

---

1. MDP：马尔可夫决策过程

   ![img](https://upload-images.jianshu.io/upload_images/10816620-463bd0065a160596.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/561)

   其中的$ R_{t+1}$ 是t时刻采取action之后立刻获得的reward

   - Bellman方程
     $$
     v(s)=R_s + \gamma\sum_{s^\prime\in S} \ P_{ss^\prime}v(s^\prime)
     $$

   - Policy
     $$
     \pi(a|s)=P[A_t=a|S_t=s]
     $$

   - Value Function
     $$
     v_\pi(s)=E_\pi[G_t | S_t=s]
     $$

   - Action-Value Function
     $$
     q_\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]
     $$

   - V函数和Q函数的关系
     $$
     v_\pi(s)=E_\pi[R_t+\gamma{v_\pi}(S_{t+1})|S_t=s]
     $$

     $$
     q_\pi(s,a)=E_\pi[R_{t+1}+\gamma{q_\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]
     $$

     通过Q函数来求V函数：
     $$
     v_\pi(s)=\sum_{a\in A}\pi(a|s)q_\pi(s,a)
     $$
     通过V函数来求Q函数：
     $$
     q_\pi(s,a)=R^a_s+\gamma\sum_{s^\prime\in S} \ P_{ss^\prime}^av_\pi(s^\prime)
     $$

   ---

2. 在学习了RL有关的知识后又重新读了学姐代码的HRL论文

3. 学习有关pytorch的内容

   - 有关Tensor的内容
   - Autograd：自动求导机制
   - 学习用torch.nn构建神经网络